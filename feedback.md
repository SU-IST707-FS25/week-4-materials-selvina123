# Assignment Feedback: Week 04 Dimensionality Reduction

**Student:** selvina123
**Raw Score:** 48/50 (96.0%)
**Course Points Earned:** 104.0

---

## Problem Breakdown

### Exercise 2 (9/10 = 90.0%)

**Part ex1-part1** (ex1-part1.code): 4/4 points

_Feedback:_ Well done. You correctly applied t-SNE to MNIST (using a reasonable 2k subset), set reproducible params, and visualized with labels and a colorbar. The plot should effectively separate digits. No issues spotted.

**Part ex1-part2** (ex1-part2.code): 2/3 points

_Feedback:_ Good attempt: you applied KNN on t-SNE features and reported accuracy. However, you fit t-SNE separately on train and test, producing incompatible spaces, so the evaluation is invalid. Fit t-SNE once (e.g., on the whole subset) then split before KNN.

**Part ex1-part3** (ex1-part3.code): 3/3 points

_Feedback:_ Correct use of UMAP features with KNN and proper accuracy calculation on test data. Consistent with prior UMAP train/test reductions. Nice work. You could optionally experiment with n_neighbors or metrics, but this meets the requirement.

---

### Exercise 4 (19/20 = 95.0%)

**Part ex2-part1** (ex2-part1.code): 7/7 points

_Feedback:_ Great job. You applied PCA (tested 2/5/10 comps), trained KNN on transformed data, reported accuracies, and provided a PCA visualization (3D scatter). This meets the task goals and appears correct. Optionally, you could also try PCA with a variance threshold (e.g., 0.9).

**Part ex2-part2** (ex2-part2.code): 7/7 points

_Feedback:_ Excellent. You correctly applied UMAP (fit on train, transform test), evaluated KNN across components, and provided a 3D visualization mirroring your PCA workflow. Solid, working implementation. Consider also a 2D plot or tuning UMAP params for insight.

**Part ex2-part3** (ex2-part3.answer): 5/6 points

_Feedback:_ Good comparison of PCA vs UMAP and why UMAP can help KNN. You explored dimensionality (2/5/10). For full credit, tie to your observed results and note that UMAP often works better in low dimensions (e.g., 2D) with low n_neighbors settings.

---

### Exercise 1 (20/20 = 100.0%)

**Part pipeline-part1** (pipeline-part1.code): 4/4 points

_Feedback:_ Great job. You correctly applied PCA to 2 components and produced a clear 2D scatter plot colored by digit class with a colorbar and labels. This meets the task requirements.

**Part pipeline-part2** (pipeline-part2.code): 4/4 points

_Feedback:_ Good job. You fit PCA on the training data, computed the first 40 components’ explained variance, converted to percent, and plotted a clear scree plot. Approach is correct and meets the requirements.

**Part pipeline-part3** (pipeline-part3.code): 4/4 points

_Feedback:_ Correct approach using prior pca_full. You computed cumulative variance and selected the first index reaching 95% with argmax+1, which satisfies the task. The extra plotting function is fine and doesn’t affect grading. Good job.

**Part pipeline-part4** (pipeline-part4.code): 4/4 points

_Feedback:_ Excellent. You correctly used n_components_95 from Step 4, applied PCA, reconstructed the digits, and visualized the same digit before and after. This meets the objective and uses prior work appropriately.

**Part pipeline-part5** (pipeline-part5.code): 4/4 points

_Feedback:_ Great job. You trained KNN with and without PCA, correctly targeted 80% variance using prior pca_full to choose n_components, transformed train/test consistently, and compared accuracies. Solid, working implementation.

---

## Additional Information

This feedback was automatically generated by the autograder.

**Generated:** 2025-10-28 19:51:30 UTC

If you have questions about your grade, please reach out to the instructor.